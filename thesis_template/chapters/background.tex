\chapter{Background}\label{chap:background}


Introduce the related state-of-the-art and background information in order to understand the method developed in the thesis. 

Automatic Machine learning which is described as the pipeline of steps, the repetitive steps are automated in order to save time and give more importance to other focus areas for an expert or to a end-user. In any application of machine learning one has to apply data pre-processing , feature extraction and feature engineering techniques to make data appropriate to be processed by any Machine learning algorithm. Once the data pre-processing to feature selection methods are applied, following is the selection of algorithms and its hyper-parameters based on the data. To gain a efficient model several optimization's has to be carried out. Automating these steps using certain probabilistic and stochastic methods such as Bayesian optimization and genetic programming gives us an advantage in gaining solutions to the problems faster and simpler. In context of aiding simple methods to solutions and improving the research on machine learning the term AutoML\cite{autoML} was coined. Under this several packages were developed some well know packages include Autosklearn\cite{autosklearn} TPOT\cite{tpot} AutoWeka\cite{autoweka-Thornton:2013:ACS:2487575.2487629} H2O AutoML. The interest of study in this thesis is on Autosklearn and TPOT since there methodologies are different and by evaluating these differences we would much gain more insight into Automatic machine learning.

\subsection{Machine Learning}

The term \blockquote{Machine learning} was coined in 1959 by Arthur Lee Samuel\footnote{\url{https://en.wikipedia.org/wiki/Arthur_Samuel}}. One of the first self-learning programs was developed by Arthur Samuel called Checkers-playing Program. The field of Artificial Intelligence had made its mark since then numerous research and development in the field of machine learning has led to where we are at the moment. One of the formal definitions of Machine learning(\textbf{ML}) was given by Tom Michael Mitchell\footnote{\url{https://en.wikipedia.org/wiki/Tom_M._Mitchell}} which states \blockquote{\textit{A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E}} from this simple definition we can have a surface understanding of how ML methodologies are developed and its applications to end-users. Humans have intelligence which enables them to think and arrive at an answer to a question. The derivation of the question and the process of tackling the possible solutions can be thought easily for a normal human brain, where as for a machine the process of handling and understanding the given tasks needs to be explicitly stated. The checkers-playing game\footnote{\url{https://en.wikipedia.org/wiki/Arthur_Samuel}} was built in such a way that every move was defined by a scoring function, for each move the outcome will be scored based on the number of pieces at the board at any position, each move was optimized in order to gain maximum chances of winning. Here the author instead of explicitly stating each move as an instruction devised a scoring function to optimize the next move reducing the computation resources without minimizing the outcome.In the early 21st century IBM developed a system called Watson\footnote{\url{https://en.wikipedia.org/wiki/Watson_(computer)}} which could answer questions posted in Natural Language, it also participated in a TV show where the participant needs to answer the questions 
asked by the host, Watson\footnote{\url{https://www.ibm.com/watson/about/}} won the contest against the current champions at that time. Compared to Arthur Lee Samuel's checkers game Watson is a highly complex AI system, still both follow the same fundamentals of AI that is there is no explicit programming before hand for the outcomes the results are generated based on a variety of Optimization functions, Statistical approaches and Algorithms. 

In this day of Computer science ML has been developed to great heights with numerous algorithms to choose from. Industrial Research and Development aiding Machine learning it has become accessible to large extent of people even for those with no understanding of programming or ML domain. Different varieties of ML has been emerged over the years  


\subsection{Bayesian Optimization}

Bayesian optimization is a method of optimizing a function which is very time consuming to produce an outcome. In our case tuning of hyper-parameters for an algorithm which is demonstrated in Autosklearn\cite{autosklearn}. In simple terms a function \textit{f(x)} has to be optimized within a number of steps, having the number of steps as a threshold is the budget or the cost \textit{c}. Bayesian optimization will take assumptions about the function \textit{f(x)} and then updates those previous assumptions with the samples drawn out from the function \textit{f(x)}, this will provide a an estimate on where to evaluate next in the upcoming iterations i.e in which part of the search space does the important hyper-parameters has to be searched. This will minimize the number of steps to reach a optimal solution.